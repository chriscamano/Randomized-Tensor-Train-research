\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{nccmath}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{mathtools,amssymb}
\usepackage{tikz}
\usepackage{xcolor}
\pgfplotsset{compat = newest}
\author{Chris Camano: ccamano@sfsu.edu}
\title{MATH425 Lecture 5 }
\date{2/10/2022}
% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\newcommand{\q}{\quadd}
\renewcommand{\labelenumi}{\alph{enumi})}
\newcommand{\rtwo}{$\mathbb{R}^2$}
\newcommand{\C}{$\mathbb{C}$}

\begin{document}
Consider the following example:

Imagine we are in the process of computing a sweep of dmrg on our ALS tensor network. \\
To proceed we mus describe the problem as a local problem by treating:
\newcommand{\Tr}{\text{trace}}
\[
   \Tr(X^TAX)\mapsto \Tr\left((X^k)^TX_{\neq k} A^k X_{\neq k}X^k \right)
\]
Where $X^k$ and $(X^k)^T$ are the cores being contracted above and below the current core of A, $A^k$\\

In this case the dimensions of the cores are as follows:
\begin{align*}
  X^k \in \mathbb{R}^{2r_{k-1}r_{k}\times K}\\
  X_{\neq k} A^k X_{\neq k} \in \mathbb{R}^{2r_{k-1}r_{k}\times 2r_{k-1}r_{k}}
\end{align*}
Where the coefficient 2 appears as the physcial leg of the core being contracted with A in the case of a hamiltonian. \\

Since the approximate eigenvector will be constructed with a fixed bond dimension $r_k=r_k-1$ so: \\
\begin{align*}
  X^k \in \mathbb{R}^{2r_{k}^2\times K}\\
  X_{\neq k} A^k X_{\neq k} \in \mathbb{R}^{2r_{k}^2\times 2r_{k}^2}
\end{align*}
\\
These observations lead to a a somewhat startling conclusion since:
\[

  (X^k)^TX_{\neq k} A^k X_{\neq k}X^k \in \mathbb{R}^{(K \times 2r_{k-1}r_{k})(2r_{k}^2\times 2r_{k}^2)(2r_{k}^2\times K)} \mapsto (X^k)^TX_{\neq k} A^k X_{\neq k}X^k \in \mathbb{R}^{K\times K}
\]

The added physcial dimension of K is typically very small, in the literature it appears to actually be 1 most of the time or slightly more to improve convergence. My understanding is the LOBPCG needs a square matrix so these dimensons check out to me which is also problematic. I need to confirm this in the matlab code to be sure but I wanted to see what you were thinking.

Also in terms of sketching with our random matrix idea, after reading through the Nystrom method literature it seems like a functional improvement over rsvd in most cases which is amazing. I have yet to draw the connection to the eigenproblem as I am still working through paper 2 but this is very interesting work and I think it might be able to be used here as a potential aternative to LOBPCG. see the sketched Rayleigh Ritz method that currently seems to be the most practical.

 Currently it seems that if we were to sketch anything to shrink the input for the local eigensolver it would have to be related to the cores of the vectors bein contracted on , but since the dimensions here suggest that we could only sketch over K I am still trying to find a solution. 

\end{document}
